# ðŸš€ MÃ³dulo 04: Model Serving con FastAPI + Spark

Aprende a servir modelos de Machine Learning entrenados con Spark en producciÃ³n mediante APIs REST de alta performance con FastAPI.

---

## ðŸŽ¯ Objetivos de Aprendizaje

Al completar este mÃ³dulo, serÃ¡s capaz de:

- âœ… Entender los fundamentos de ML Model Serving y deployment
- âœ… Construir APIs REST de producciÃ³n con FastAPI
- âœ… Servir predicciones desde archivos Parquet generados por Spark
- âœ… Implementar caching en memoria para baja latencia (<50ms)
- âœ… DiseÃ±ar endpoints para batch y real-time inference
- âœ… Implementar health checks y monitoring
- âœ… Manejar diferentes patrones de serving (batch, online, streaming)
- âœ… Optimizar performance para alta throughput y baja latencia

---

## ðŸ“š Fundamentos TeÃ³ricos

### Â¿QuÃ© es Model Serving?

**Model Serving** es el proceso de hacer que un modelo de ML entrenado estÃ© disponible para hacer predicciones en producciÃ³n sobre nuevos datos.

```
Training (Offline)          â†’          Serving (Online)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Historical Data                        New Data
       â†“                                     â†“
[Train Model]                          [Load Model]
       â†“                                     â†“
Save to Storage                  [Make Prediction]
(MLflow/S3/DB)                              â†“
                                      Return Result
```

### El Problema del "Last Mile" en ML

**80% del tiempo en ML** se gasta en entrenamiento e investigaciÃ³n. **Solo 20% en deployment**, pero es donde el negocio obtiene valor.

```
Jupyter Notebook (90% accuracy) â†’ âœ… Funciona
                â†“
Model en producciÃ³n (Â¿?)        â†’ âŒ No disponible = $0 valor
```

**Model Serving** cierra esta brecha.

---

## ðŸ—ï¸ Arquitecturas de Model Serving

### 1. Batch Serving (Offline Predictions)

**CuÃ¡ndo usar:** Predicciones que no necesitan ser inmediatas

```
                Scheduled Job
                     â†“
[Spark Batch] â†’ Generate predictions
                     â†“
              Save to Database
                     â†“
            [API] â†’ Read from DB
                     â†“
                Application
```

**Ejemplo:** Scoring diario de 1M leads
- âœ… Pro: EconÃ³mico, puede procesar volÃºmenes masivos
- âŒ Con: Latencia de horas/dÃ­as, predicciones "stale"

**ImplementaciÃ³n:**
```python
# Spark genera predicciones cada noche
predictions_df = model.transform(all_leads)
predictions_df.write.parquet("s3://predictions/date=2024-01-15/")

# API lee predicciones pre-calculadas
@app.get("/predictions/{lead_id}")
def get_prediction(lead_id: int):
    # Leer de cache/DB
    return cached_predictions[lead_id]
```

### 2. Online Serving (Real-time Predictions)

**CuÃ¡ndo usar:** Predicciones inmediatas (< 100ms latency)

```
User Request â†’ [API] â†’ [Load Model] â†’ Predict â†’ Response
                            â†‘
                      Model in Memory
```

**Ejemplo:** RecomendaciÃ³n de productos al navegar
- âœ… Pro: Latencia baja, predicciones actualizadas
- âŒ Con: Mayor costo compute, lÃ­mites de throughput

### 3. Cache Pattern (Usado en este mÃ³dulo)

Predicciones pre-calculadas en cache:

```
Spark Batch    FastAPI with Cache
    â†“               â†“
[Generate]     [Load Parquet]
predictions         â†“
    â†“          [In-Memory Cache]
Parquet             â†“
               Serve <50ms
```

**Pros:**
- âœ… Latencia ultra-baja
- âœ… Throughput alto
- âœ… Simple y econÃ³mico

**Cons:**
- âŒ Solo para predicciones pre-calculadas
- âŒ Requiere refresh periÃ³dico

---

## ðŸš€ FastAPI: El Framework Moderno

### Â¿Por quÃ© FastAPI?

**FastAPI** es el framework web Python de mÃ¡s rÃ¡pido crecimiento, especialmente popular para ML serving.

| Feature | FastAPI | Flask | Django |
|---------|---------|-------|--------|
| **Performance** | âš¡âš¡âš¡ (Async) | âš¡ (Sync) | âš¡ (Sync) |
| **DocumentaciÃ³n automÃ¡tica** | âœ… OpenAPI/Swagger | âŒ | âŒ |
| **Type validation** | âœ… Pydantic | âŒ | Parcial |
| **Async support** | âœ… Nativo | âš¡ Limitado | âš¡ Limitado |
| **ML Serving** | â­â­â­â­â­ | â­â­â­ | â­â­ |

**Benchmark (requests/second):**
```
FastAPI (async): 20,000+ req/s
Flask:            2,000 req/s
Django:           1,500 req/s
```

---

## ðŸ› ï¸ API Endpoints

### GET /predictions/{lead_id} - PredicciÃ³n Individual

```bash
curl http://localhost:8000/predictions/42

# Response:
{
  "lead_id": 42,
  "conversion_probability": 0.8234,
  "segment": "Hot Lead",
  "timestamp": "2024-01-15T10:30:00"
}
```

### GET /predictions - Batch con Filtros

```bash
# Top 10 hot leads
curl "http://localhost:8000/predictions?segment=hot&limit=10"

# Leads con prob > 0.5
curl "http://localhost:8000/predictions?min_probability=0.5&limit=50"
```

### POST /predict - Inferencia en Tiempo Real

```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "age": 35,
    "salary": 75000,
    "web_visits": 15,
    "last_action": "form_submit"
  }'

# Response:
{
  "conversion_probability": 0.82,
  "predicted_conversion": 1,
  "segment": "Hot Lead"
}
```

### GET /stats - Analytics

```bash
curl http://localhost:8000/stats

# Response:
{
  "total_leads": 1000,
  "avg_probability": 0.45,
  "segments": {
    "hot_leads": 150,
    "warm_leads": 350,
    "cold_leads": 500
  }
}
```

### GET /health - Health Check

```bash
curl http://localhost:8000/health

# Response:
{
  "status": "healthy",
  "predictions_loaded": true,
  "predictions_count": 1000,
  "cache_age_seconds": 3600
}
```

---

## ðŸš€ Inicio RÃ¡pido

### 1. Generar Predicciones con Spark

```bash
# Primero genera datos
python scripts/generate_data.py

# Entrena modelo y genera predicciones
python 02-03-ML/lead_scoring.py

# Verifica que se generÃ³ el archivo
ls -lh data/lead_predictions.parquet
```

### 2. Iniciar API

```bash
# Instalar dependencias
pip install fastapi uvicorn pandas pyarrow

# Iniciar servidor
python 04-FastAPI-Serving/api.py

# O con uvicorn
uvicorn api:app --reload --host 0.0.0.0 --port 8000
```

### 3. Probar API

```bash
# Abrir documentaciÃ³n interactiva
open http://localhost:8000/docs

# O probar endpoints
curl http://localhost:8000/health
curl http://localhost:8000/predictions/1
curl http://localhost:8000/stats
```

---

## ðŸ“Š Performance

### Latency Targets

| Endpoint | Target | Actual |
|----------|--------|--------|
| GET /predictions/{id} | <50ms | ~5ms |
| GET /predictions (batch) | <200ms | ~50ms |
| POST /predict (realtime) | <100ms | ~30ms |
| GET /stats | <100ms | ~20ms |

### Optimizaciones Implementadas

1. **In-Memory Cache**: Predicciones cargadas en RAM (pandas DataFrame)
2. **Indexed Lookups**: O(1) bÃºsqueda por lead_id
3. **ORJSON**: SerializaciÃ³n JSON 2x mÃ¡s rÃ¡pida
4. **Async Endpoints**: No bloquean el event loop

---

## ðŸ” Monitoring

### MÃ©tricas Disponibles

- Total de requests por endpoint
- Latencia promedio, p50, p95, p99
- Cache hit rate
- Error rate por tipo
- Edad del cache

### IntegraciÃ³n con Prometheus

```python
from prometheus_client import Counter, Histogram

prediction_requests = Counter(
    'prediction_requests_total',
    'Total prediction requests',
    ['endpoint', 'status']
)

@app.get("/metrics")
async def metrics():
    return generate_latest()
```

---

## ðŸ³ Deployment

### Docker

```dockerfile
FROM python:3.10-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000"]
```

```bash
docker build -t lead-scoring-api .
docker run -p 8000:8000 -v $(pwd)/data:/app/data lead-scoring-api
```

### Kubernetes

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lead-scoring-api
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: api
        image: lead-scoring-api:v1.0
        ports:
        - containerPort: 8000
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
```

---

## ðŸ’¡ Mejores PrÃ¡cticas

### 1. Caching EstratÃ©gico

```python
# BIEN: Cache global cargado al iniciar
predictions_cache = pd.read_parquet("predictions.parquet")

# MAL: Leer en cada request
df = pd.read_parquet("predictions.parquet")  # âŒ Muy lento
```

### 2. ValidaciÃ³n con Pydantic

```python
from pydantic import BaseModel, Field

class LeadInput(BaseModel):
    age: int = Field(..., ge=18, le=100)
    salary: float = Field(..., gt=0)
    web_visits: int = Field(..., ge=0)
```

### 3. Rate Limiting

```python
from slowapi import Limiter

limiter = Limiter(key_func=get_remote_address)

@app.get("/predictions/{lead_id}")
@limiter.limit("100/minute")
async def get_prediction(lead_id: int):
    ...
```

### 4. Logging Estructurado

```python
logger.info(json.dumps({
    "event": "prediction_made",
    "lead_id": lead_id,
    "probability": prob,
    "latency_ms": latency
}))
```

---

## ðŸ“š Referencias

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Pydantic Models](https://pydantic-docs.helpmanual.io/)
- [ML Model Serving Patterns](https://www.kdnuggets.com/2021/01/ml-model-serving-patterns.html)

---

**Â¡Siguiente paso! ðŸ‘‰ [MÃ³dulo 08: Streaming ML](../08-Spark-Streaming/README.md) para scoring en tiempo real con Kafka**

