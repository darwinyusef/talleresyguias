# ğŸ“Š MÃ³dulo 01: ETL - de CSV a Parquet con Spark

En este mÃ³dulo aprenderemos por quÃ© Parquet es el estÃ¡ndar de oro en Big Data y cÃ³mo convertir datos eficientemente usando Apache Spark.

## ğŸ¯ Objetivos de Aprendizaje

- Entender las ventajas del formato Parquet vs CSV
- Implementar un pipeline ETL completo con PySpark
- Aplicar transformaciones y limpieza de datos
- Manejar particionamiento de datos para optimizar consultas
- Comprender el concepto de Shuffle en Spark

## ğŸ“š Â¿Por quÃ© Parquet?

Parquet es un formato de almacenamiento columnar optimizado para Big Data:

| CaracterÃ­stica | CSV | Parquet |
|---------------|-----|---------|
| **Formato** | Basado en filas | Columnar |
| **CompresiÃ³n** | MÃ­nima | 80-90% de reducciÃ³n |
| **Lectura selectiva** | Lee todo el archivo | Solo columnas necesarias |
| **Esquema** | Sin tipo de datos | Esquema embebido |
| **Performance en Analytics** | Lenta | Muy rÃ¡pida |

### Ventajas Principales:
- **Formato Columnar:** Solo lee las columnas necesarias, ahorrando I/O masivamente
- **CompresiÃ³n Eficiente:** Reduce drÃ¡sticamente el tamaÃ±o en disco (tÃ­picamente 80-90%)
- **Esquema Embebido:** Guarda metadatos y tipos de datos automÃ¡ticamente
- **Compatible:** Funciona con todo el ecosistema Big Data (Spark, Hive, Presto, etc.)
- **Predicate Pushdown:** Filtra datos antes de leerlos completamente

## ğŸ› ï¸ Scripts Incluidos

### 1. `etl_csv_to_parquet.py` - ETL Completo ProducciÃ³n

Script completo y robusto para ETL en ambientes reales:

**Funcionalidades:**
- âœ… Lectura de CSV con manejo de errores
- âœ… Transformaciones de limpieza de datos
- âœ… NormalizaciÃ³n de texto
- âœ… Manejo de valores nulos
- âœ… CreaciÃ³n de columnas derivadas
- âœ… Escritura optimizada en Parquet
- âœ… Particionamiento inteligente
- âœ… ComparaciÃ³n de tamaÃ±os de archivo
- âœ… ValidaciÃ³n de datos

**EjecuciÃ³n:**
```bash
# Primero genera los datos de prueba
python scripts/generate_data.py

# Ejecuta el ETL
python 01-ETL-CSV-Parquet/etl_csv_to_parquet.py
```

**ConfiguraciÃ³n Personalizable:**
```python
INPUT_PATH = "data/lead_conversions.csv"
OUTPUT_PATH = "data/processed_leads.parquet"
PARTITION_COLUMN = "salary_range"  # Opcional
```

### 2. `big_data_demo.py` - Demo de Conceptos Avanzados

Demuestra conceptos crÃ­ticos de Big Data a escala:

**Conceptos Demostrados:**
1. **GeneraciÃ³n Masiva:** Crea 10 millones de filas in-memory
2. **Shuffle Operations:** Muestra cÃ³mo las agregaciones mueven datos entre nodos
3. **Partitioning:** Compara performance entre datos planos vs particionados
4. **Partition Pruning:** OptimizaciÃ³n automÃ¡tica de lectura

**EjecuciÃ³n:**
```bash
python 01-ETL-CSV-Parquet/big_data_demo.py
```

**Salida Esperada:**
```
ğŸš€ Generando 10 millones de filas...
ğŸ“Š Estado inicial de particiones: 8
â±ï¸ Tiempo procesado (sin particionar): 12.34s
ğŸ’¾ Guardando datos particionados...
ğŸ” Demostrando Partition Pruning...
â±ï¸ Tiempo de conteo con filtro de particiÃ³n: 0.0123s
```

## ğŸ”‘ Conceptos Clave

### Particionamiento FÃ­sico

El particionamiento divide los datos en subdirectorios en disco:

```
data/partitioned_data.parquet/
â”œâ”€â”€ category=0/
â”‚   â””â”€â”€ part-00000.parquet
â”œâ”€â”€ category=1/
â”‚   â””â”€â”€ part-00001.parquet
â””â”€â”€ category=2/
    â””â”€â”€ part-00002.parquet
```

**Ventaja:** Cuando filtras por `category=1`, Spark solo lee ese directorio.

### Shuffle en Spark

**Shuffle** es el movimiento de datos entre particiones/nodos:

```python
# Esto causa un shuffle (datos se reorganizan por 'category')
df.groupBy("category").agg(sum("value"))
```

**CuÃ¡ndo ocurre:**
- `groupBy()`, `join()`, `distinct()`, `repartition()`

**Costo:**
- I/O de red
- SerializaciÃ³n/DeserializaciÃ³n
- Escritura temporal en disco

## ğŸ“– Ejemplo Paso a Paso

### 1. Crear SesiÃ³n de Spark
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("ETL-CSV-Parquet") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()
```

### 2. Leer CSV
```python
df = spark.read.csv(
    "data/lead_conversions.csv",
    header=True,
    inferSchema=True,
    nullValue="NA"
)
```

### 3. Transformaciones
```python
from pyspark.sql.functions import col, when, trim

# Limpiar datos
df_clean = df.filter(col("id").isNotNull()) \
    .withColumn("age", col("age").cast("integer")) \
    .fillna({"salary": 0})

# Crear columna derivada
df_clean = df_clean.withColumn(
    "salary_range",
    when(col("salary") < 30000, "Low")
    .when(col("salary") < 70000, "Medium")
    .otherwise("High")
)
```

### 4. Guardar como Parquet
```python
# Sin particiones
df_clean.write.mode("overwrite") \
    .option("compression", "snappy") \
    .parquet("data/output.parquet")

# Con particiones (recomendado para datos grandes)
df_clean.write.mode("overwrite") \
    .partitionBy("salary_range") \
    .parquet("data/output_partitioned.parquet")
```

### 5. Leer Parquet
```python
# Lectura simple
df_parquet = spark.read.parquet("data/output.parquet")

# Lectura con filtro (aprovecha partition pruning)
df_filtered = spark.read.parquet("data/output_partitioned.parquet") \
    .filter(col("salary_range") == "High")
```

## ğŸ’¡ Mejores PrÃ¡cticas

1. **Usa CompresiÃ³n Snappy:** Balance entre velocidad y ratio de compresiÃ³n
   ```python
   .option("compression", "snappy")
   ```

2. **Particiona Inteligentemente:**
   - Usa columnas con cardinalidad media (10-10,000 valores Ãºnicos)
   - Evita sobre-particionamiento (archivos muy pequeÃ±os)
   - Columnas tÃ­picas: fecha, regiÃ³n, categorÃ­a

3. **Habilita Adaptive Query Execution:**
   ```python
   .config("spark.sql.adaptive.enabled", "true")
   ```

4. **Evita Particiones Muy PequeÃ±as:**
   - Archivos < 100MB son ineficientes
   - Usa `coalesce()` para reducir particiones si es necesario

5. **Infiere Schema con Cuidado:**
   - `inferSchema=True` lee el archivo dos veces
   - En producciÃ³n, define el schema explÃ­citamente

## ğŸš€ PrÃ³ximos Pasos

Una vez que domines este mÃ³dulo, continÃºa con:
- **[MÃ³dulo 02-03: Machine Learning](../02-03-ML/README.md)** - Entrena modelos sobre datos Parquet
- **[MÃ³dulo 06: S3/MinIO](../06-Storage-S3-MinIO/README.md)** - Almacena Parquet en object storage

## ğŸ“š Referencias

- [DocumentaciÃ³n oficial de Parquet](https://parquet.apache.org/)
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Best Practices for Writing Parquet](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)

