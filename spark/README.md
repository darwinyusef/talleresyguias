# âš¡ Taller de Apache Spark: De Cero a MLOps + Streaming

Bienvenidos a este taller integral de **Apache Spark**. AprenderÃ¡s desde los conceptos bÃ¡sicos hasta arquitecturas de producciÃ³n modernas con Machine Learning, Streaming en tiempo real, y deployment con Docker/Kubernetes.

## ğŸš€ Â¿QuÃ© es Apache Spark?

**Apache Spark** es un motor de procesamiento de datos distribuido unificado y de cÃ³digo abierto, diseÃ±ado para velocidad y escala. Procesa datos **100x mÃ¡s rÃ¡pido** que Hadoop MapReduce gracias a su procesamiento en memoria.

### ğŸ’¡ Â¿Para quÃ© sirve?

| Caso de Uso | TecnologÃ­a Spark | AplicaciÃ³n Real |
|-------------|------------------|-----------------|
| **ETL a Escala** | Spark Core + SQL | Procesar petabytes de logs diarios |
| **Machine Learning** | MLlib | Modelos de recomendaciÃ³n, fraud detection |
| **Streaming en Tiempo Real** | Structured Streaming | Alertas IoT, trading algorithms |
| **Analytics Interactivo** | Spark SQL | Consultas sobre Data Lakes |
| **Procesamiento de Grafos** | GraphX | Redes sociales, detecciÃ³n de comunidades |

### ğŸ¯ Â¿Por quÃ© aprender Spark en 2024?

- âœ… **EstÃ¡ndar de facto** para Big Data en empresas Fortune 500
- âœ… **Unifica batch y streaming** (misma API para ambos)
- âœ… **Ecosistema rico**: IntegraciÃ³n nativa con AWS, Azure, GCP, Databricks
- âœ… **Salario promedio**: $130K+ para Spark Engineers
- âœ… **Open Source**: Sin vendor lock-in

---

## ğŸ› ï¸ Estructura del Taller

El taller estÃ¡ diseÃ±ado como un **journey progresivo** desde fundamentos hasta arquitecturas de producciÃ³n:

### ğŸ“š Fundamentos (Batch Processing)

1.  **[01-ETL-CSV-Parquet](./01-ETL-CSV-Parquet/README.md)**
    ğŸ“ **Nivel**: Principiante
    ğŸ“Š OptimizaciÃ³n de almacenamiento con formato columnar Parquet
    ğŸ”‘ **Conceptos**: DataFrames, transformaciones, particionamiento

2.  **[02-03-ML](./02-03-ML/README.md)**
    ğŸ“ **Nivel**: Intermedio
    ğŸ¤– Machine Learning distribuido con MLlib + MLflow
    ğŸ”‘ **Conceptos**: Pipelines ML, Feature Engineering, Model Tracking
    ğŸ“ **Incluye**:
    - `lead_scoring.py` - ClasificaciÃ³n supervisada (GBT, Random Forest, Logistic Regression)
    - `customer_segmentation.py` - Clustering no supervisado (K-Means, Elbow Method)

### ğŸŒ IntegraciÃ³n y Serving

3.  **[04-FastAPI-Serving](./04-FastAPI-Serving/README.md)**
    ğŸ“ **Nivel**: Intermedio
    ğŸš€ API REST para servir predicciones de modelos Spark
    ğŸ”‘ **Conceptos**: ML Serving, caching, batch predictions
    ğŸ“ **Features**:
    - Lectura de Parquet generado por Spark
    - Endpoints de predicciÃ³n batch y realtime
    - Health checks y mÃ©tricas

### âš¡ Procesamiento en Tiempo Real (NUEVO)

4.  **[08-Spark-Streaming](./08-Spark-Streaming/README.md)** â­ **DESTACADO**
    ğŸ“ **Nivel**: Avanzado
    ğŸ“¡ Structured Streaming para procesamiento en tiempo real
    ğŸ”‘ **Conceptos**: Windowing, watermarks, Kafka integration, streaming ML
    ğŸ“ **Scripts incluidos**:
    - `streaming_ml_scoring.py` - **ML scoring en tiempo real**
    - `kafka_producer.py` - Generador de eventos de leads
    - `streaming_word_count.py` - Hello World de streaming
    - `streaming_aggregations.py` - Ventanas temporales y agregaciones

### ğŸ—ï¸ Infraestructura y OrquestaciÃ³n

5.  **[05-Airflow](./05-Airflow/README.md)**
    ğŸ“ **Nivel**: Intermedio
    ğŸ—“ï¸ OrquestaciÃ³n de pipelines Spark con Apache Airflow
    ğŸ”‘ **Conceptos**: DAGs, scheduling, dependency management

6.  **[06-Storage-S3-MinIO](./06-Storage-S3-MinIO/README.md)**
    ğŸ“ **Nivel**: Intermedio
    â˜ï¸ Data Lake con S3-compatible storage
    ğŸ”‘ **Conceptos**: Bronze/Silver/Gold architecture, incremental loads
    ğŸ“ **Incluye**: `spark_s3_integration.py` con ejemplos completos

7.  **[07-Docker-K8s](./07-Docker-K8s/README.md)**
    ğŸ“ **Nivel**: Avanzado
    ğŸ³ Deployment en producciÃ³n con Kubernetes
    ğŸ”‘ **Conceptos**: ContainerizaciÃ³n, Spark Operator, auto-scaling

---

## ğŸ Inicio RÃ¡pido (Quick Start)

### OpciÃ³n 1: Setup Local (Recomendado para aprender)

#### Requisitos Previos
- **Python 3.9+** (verificar: `python --version`)
- **Java 11+** (requerido para Spark runtime: `java -version`)
- **Docker Desktop** (opcional, para infraestructura)

#### Pasos:

```bash
# 1. Clonar o navegar al directorio del taller
cd spark/

# 2. Instalar dependencias Python
pip install pyspark==3.4.0 mlflow fastapi uvicorn pandas pyarrow

# Para streaming (opcional):
pip install kafka-python

# 3. Generar datos de ejemplo
python scripts/generate_data.py

# 4. Ejecutar tu primer script de Spark
python 01-ETL-CSV-Parquet/etl_csv_to_parquet.py

# 5. Ver los resultados
ls -lh data/processed_leads.parquet
```

#### Verificar InstalaciÃ³n:

```bash
# Test Spark
python -c "from pyspark.sql import SparkSession; print('Spark OK')"

# Test MLflow
mlflow --version
```

### OpciÃ³n 2: Infraestructura Completa con Docker

Para acceder a **todo** el ecosistema (MinIO, PostgreSQL, MLflow, Airflow):

```bash
# Levantar servicios
docker-compose up -d

# Verificar que estÃ©n corriendo
docker ps

# Acceder a UIs:
# - MLflow: http://localhost:5000
# - MinIO Console: http://localhost:9001 (user/pass: minioadmin/minioadmin)
# - Airflow: http://localhost:8081
# - Spark Master UI: http://localhost:8080
```

**Nota**: Docker Compose puede requerir recursos significativos. Para aprendizaje inicial, usa OpciÃ³n 1.

---

## ğŸ“ Rutas de Aprendizaje Recomendadas

### ğŸŸ¢ Para Principiantes en Spark

**Objetivo**: Entender fundamentos de procesamiento distribuido

```
DÃ­a 1-2: MÃ³dulo 01 (ETL CSVâ†’Parquet)
  â†“
DÃ­a 3-4: MÃ³dulo 02-03 (Machine Learning bÃ¡sico)
  â†“
DÃ­a 5: MÃ³dulo 04 (FastAPI Serving)
```

**Habilidades obtenidas**: DataFrames, transformaciones, ML pipelines, serving

### ğŸŸ¡ Para Data Engineers

**Objetivo**: Arquitecturas de producciÃ³n end-to-end

```
MÃ³dulo 01 â†’ MÃ³dulo 06 (S3/MinIO Data Lake)
  â†“
MÃ³dulo 08 (Streaming) â† CRÃTICO
  â†“
MÃ³dulo 05 (Airflow para orquestaciÃ³n)
  â†“
MÃ³dulo 07 (Deployment K8s)
```

**Habilidades obtenidas**: Data Lakes, streaming pipelines, orquestaciÃ³n, deployment

### ğŸ”´ Para ML Engineers

**Objetivo**: MLOps y serving de modelos a escala

```
MÃ³dulo 01 â†’ MÃ³dulo 02-03 (ML + MLflow)
  â†“
MÃ³dulo 04 (FastAPI Serving)
  â†“
MÃ³dulo 08 (Streaming ML - scoring en tiempo real)
  â†“
MÃ³dulo 05 (Re-training con Airflow)
```

**Habilidades obtenidas**: Feature engineering distribuido, model tracking, serving, online learning

---

## ğŸ—ï¸ Arquitecturas de Referencia

### Arquitectura 1: Batch ML Pipeline (BÃ¡sica)

```
Raw Data (CSV)
    â†“
[Spark ETL] â†’ Parquet (Data Lake)
    â†“
[Spark MLlib] â†’ Train Model
    â†“
[MLflow] â†’ Model Registry
    â†“
[FastAPI] â†’ Serving Layer
```

**MÃ³dulos**: 01 â†’ 02-03 â†’ 04

### Arquitectura 2: Real-time ML Scoring (Avanzada)

```
Events (Kafka)
    â†“
[Spark Streaming] â†’ Parse & Enrich
    â†“
[ML Model (MLflow)] â†’ Real-time Scoring
    â†“
Hot Leads â†’ [Kafka] â†’ CRM Integration
All Predictions â†’ [S3/MinIO] â†’ Analytics
```

**MÃ³dulos**: 02-03 â†’ 06 â†’ 08 â­

### Arquitectura 3: Medallion Architecture (ProducciÃ³n)

```
Sources â†’ [Bronze Layer (Raw)] â†’ S3/MinIO
              â†“
         [Spark ETL]
              â†“
         [Silver Layer (Cleaned)] â†’ S3/MinIO
              â†“
         [Spark Aggregations]
              â†“
         [Gold Layer (Analytics)] â†’ S3/MinIO
              â†“
         [FastAPI / BI Tools]
```

**MÃ³dulos**: 01 â†’ 06 â†’ 04

### Arquitectura 4: Complete MLOps Platform

```
                    â”Œâ”€ Batch Jobs (Airflow)
                    â”‚
Data Sources â†’ [Kafka] â†’ Spark Streaming
                    â”‚         â†“
                    â”‚    Feature Store (S3)
                    â”‚         â†“
                    â””â†’ ML Training (Spark MLlib)
                              â†“
                         MLflow Registry
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â†“                   â†“
            Batch Inference      Real-time API
            (Spark Batch)         (FastAPI)
                    â†“                   â†“
                Application / Dashboard
```

**MÃ³dulos**: TODOS (Proyecto final)

---

## ğŸ“¦ Datasets Incluidos

Este taller incluye datasets sintÃ©ticos realistas:

1. **`lead_conversions.csv`** (1000 filas)
   - Leads de CRM con caracterÃ­sticas demogrÃ¡ficas
   - Features: age, salary, web_visits, last_action
   - Target: converted (0/1)
   - Uso: MÃ³dulos 01, 02-03, 04, 08

2. **Eventos de streaming** (generados en tiempo real)
   - Producidos por `kafka_producer.py`
   - Simula eventos de leads con timestamps reales
   - Uso: MÃ³dulo 08

---

## ğŸ’¡ Mejores PrÃ¡cticas del Taller

### âœ… DO's (Haz esto)

- âœ… **Ejecuta cada script** - No solo leas, ejecuta el cÃ³digo
- âœ… **Experimenta** - Modifica parÃ¡metros, rompe cosas, aprende
- âœ… **Lee los comentarios** - Cada script estÃ¡ documentado
- âœ… **Revisa los logs** - Entiende quÃ© hace Spark internamente
- âœ… **Usa Spark UI** - http://localhost:4040 cuando ejecutes scripts

### âŒ DON'Ts (Evita esto)

- âŒ No saltarte mÃ³dulos - son progresivos
- âŒ No copiar/pegar sin entender
- âŒ No usar `collect()` en producciÃ³n con datos grandes
- âŒ No hardcodear credentials en cÃ³digo

---

## ğŸš€ PrÃ³ximos Pasos DespuÃ©s del Taller

Una vez completado el taller, continÃºa tu aprendizaje con:

1. **Databricks Academy** (gratis) - CertificaciÃ³n Apache Spark
2. **Spark: The Definitive Guide** (libro) - O'Reilly
3. **Proyecto Real**: Implementa un pipeline end-to-end con tus propios datos
4. **Contribuye a Open Source**: Apache Spark o proyectos relacionados
5. **EspecialÃ­zate**:
   - Delta Lake para ACID en Data Lakes
   - Apache Iceberg / Apache Hudi
   - Databricks / AWS EMR / GCP Dataproc
   - Ray para distributed ML mÃ¡s avanzado

---

## ğŸ¤ Contribuciones y Feedback

Este taller es de cÃ³digo abierto y estÃ¡ en constante mejora:

- ğŸ› **Issues**: Reporta bugs o mejoras
- ğŸ’¡ **Ideas**: PropÃ³n nuevos mÃ³dulos
- ğŸ“ **PRs**: Contribuye con cÃ³digo o documentaciÃ³n

---

## ğŸ“š Recursos Adicionales

- [DocumentaciÃ³n Oficial Spark](https://spark.apache.org/docs/latest/)
- [Spark By Examples](https://sparkbyexamples.com/)
- [Awesome Spark](https://github.com/awesome-spark/awesome-spark)
- [Stack Overflow - Tag: apache-spark](https://stackoverflow.com/questions/tagged/apache-spark)

---

## ğŸ“„ Licencia

Este taller es de cÃ³digo abierto y uso educativo.

---

**Â¡Comencemos! ğŸ‘‰ [MÃ³dulo 01: ETL CSV a Parquet](./01-ETL-CSV-Parquet/README.md)**

