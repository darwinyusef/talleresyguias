{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo 1.2: Pipelines de Clasificación con MLflow\n",
    "\n",
    "## Objetivos\n",
    "- Crear pipelines de preprocesamiento reproducibles\n",
    "- Tracking de pipelines completos con MLflow\n",
    "- Comparar múltiples algoritmos de clasificación\n",
    "- Nested runs para organizar experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer, load_wine\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"sklearn-classification-pipelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset: Breast Cancer Wisconsin\n",
    "\n",
    "Dataset de clasificación binaria para detección de cáncer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df['target'] = y\n",
    "df['diagnosis'] = df['target'].map({0: 'malignant', 1: 'benign'})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['diagnosis'].value_counts())\n",
    "print(f\"\\nClass balance: {df['diagnosis'].value_counts(normalize=True)}\")\n",
    "\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Train positive rate: {y_train.mean():.3f}\")\n",
    "print(f\"Test positive rate: {y_test.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipelines con scikit-learn\n",
    "\n",
    "Un pipeline encadena preprocesamiento y modelo en un solo objeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(scaler_type='standard', model=None):\n",
    "    \n",
    "    scalers = {\n",
    "        'standard': StandardScaler(),\n",
    "        'minmax': MinMaxScaler(),\n",
    "        'robust': RobustScaler()\n",
    "    }\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', scalers[scaler_type]),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(pipeline, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'train_accuracy': accuracy_score(y_train, y_pred_train),\n",
    "        'test_accuracy': accuracy_score(y_test, y_pred_test),\n",
    "        'precision': precision_score(y_test, y_pred_test),\n",
    "        'recall': recall_score(y_test, y_pred_test),\n",
    "        'f1_score': f1_score(y_test, y_pred_test),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    cv_scores = cross_val_score(\n",
    "        pipeline, X_train, y_train, cv=5, scoring='accuracy'\n",
    "    )\n",
    "    metrics['cv_accuracy_mean'] = cv_scores.mean()\n",
    "    metrics['cv_accuracy_std'] = cv_scores.std()\n",
    "    \n",
    "    return metrics, y_pred_test, y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimento 1: Comparar Scalers\n",
    "\n",
    "Vamos a comparar diferentes métodos de escalado con el mismo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "scalers_to_test = ['standard', 'minmax', 'robust']\n",
    "\n",
    "with mlflow.start_run(run_name=\"scaler_comparison\") as parent_run:\n",
    "    \n",
    "    mlflow.set_tag(\"experiment_type\", \"scaler_comparison\")\n",
    "    mlflow.log_param(\"base_model\", \"LogisticRegression\")\n",
    "    \n",
    "    scaler_results = []\n",
    "    \n",
    "    for scaler_name in scalers_to_test:\n",
    "        with mlflow.start_run(run_name=f\"scaler_{scaler_name}\", nested=True):\n",
    "            \n",
    "            pipeline = create_pipeline(scaler_type=scaler_name, model=base_model)\n",
    "            \n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            mlflow.log_param(\"scaler\", scaler_name)\n",
    "            mlflow.log_param(\"model\", \"LogisticRegression\")\n",
    "            \n",
    "            metrics, y_pred, y_proba = evaluate_model(\n",
    "                pipeline, X_train, y_train, X_test, y_test\n",
    "            )\n",
    "            \n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(metric_name, metric_value)\n",
    "            \n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f'Confusion Matrix - {scaler_name} scaler')\n",
    "            plt.ylabel('True')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.savefig(f'cm_{scaler_name}.png')\n",
    "            mlflow.log_artifact(f'cm_{scaler_name}.png')\n",
    "            plt.close()\n",
    "            \n",
    "            mlflow.sklearn.log_model(pipeline, f\"pipeline_{scaler_name}\")\n",
    "            \n",
    "            scaler_results.append({\n",
    "                'scaler': scaler_name,\n",
    "                **metrics\n",
    "            })\n",
    "            \n",
    "            print(f\"{scaler_name}: Accuracy={metrics['test_accuracy']:.4f}, \"\n",
    "                  f\"ROC-AUC={metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(scaler_results)\n",
    "    results_df.to_csv('scaler_comparison.csv', index=False)\n",
    "    mlflow.log_artifact('scaler_comparison.csv')\n",
    "    \n",
    "    print(\"\\nComparación de Scalers:\")\n",
    "    print(results_df[['scaler', 'test_accuracy', 'roc_auc', 'f1_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimento 2: Comparar Múltiples Algoritmos\n",
    "\n",
    "Ahora comparemos diferentes algoritmos de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_test = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'NaiveBayes': GaussianNB()\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=\"model_comparison\") as parent_run:\n",
    "    \n",
    "    mlflow.set_tag(\"experiment_type\", \"model_comparison\")\n",
    "    mlflow.log_param(\"scaler\", \"standard\")\n",
    "    mlflow.log_param(\"num_models\", len(models_to_test))\n",
    "    \n",
    "    model_results = []\n",
    "    \n",
    "    for model_name, model in models_to_test.items():\n",
    "        with mlflow.start_run(run_name=f\"model_{model_name}\", nested=True):\n",
    "            \n",
    "            pipeline = create_pipeline(scaler_type='standard', model=model)\n",
    "            \n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            mlflow.log_param(\"model\", model_name)\n",
    "            mlflow.log_param(\"scaler\", \"standard\")\n",
    "            \n",
    "            if hasattr(model, 'get_params'):\n",
    "                model_params = model.get_params()\n",
    "                for param_name, param_value in model_params.items():\n",
    "                    if isinstance(param_value, (int, float, str, bool)):\n",
    "                        mlflow.log_param(f\"model_{param_name}\", param_value)\n",
    "            \n",
    "            metrics, y_pred, y_proba = evaluate_model(\n",
    "                pipeline, X_train, y_train, X_test, y_test\n",
    "            )\n",
    "            \n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(metric_name, metric_value)\n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {metrics['roc_auc']:.3f})\")\n",
    "            plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'ROC Curve - {model_name}')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'roc_{model_name}.png')\n",
    "            mlflow.log_artifact(f'roc_{model_name}.png')\n",
    "            plt.close()\n",
    "            \n",
    "            mlflow.sklearn.log_model(pipeline, f\"pipeline_{model_name}\")\n",
    "            \n",
    "            model_results.append({\n",
    "                'model': model_name,\n",
    "                **metrics\n",
    "            })\n",
    "            \n",
    "            print(f\"{model_name}: Accuracy={metrics['test_accuracy']:.4f}, \"\n",
    "                  f\"F1={metrics['f1_score']:.4f}, ROC-AUC={metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(model_results)\n",
    "    results_df = results_df.sort_values('test_accuracy', ascending=False)\n",
    "    results_df.to_csv('model_comparison.csv', index=False)\n",
    "    mlflow.log_artifact('model_comparison.csv')\n",
    "    \n",
    "    best_model = results_df.iloc[0]['model']\n",
    "    best_accuracy = results_df.iloc[0]['test_accuracy']\n",
    "    mlflow.log_param(\"best_model\", best_model)\n",
    "    mlflow.log_metric(\"best_accuracy\", best_accuracy)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Ranking de Modelos por Accuracy:\")\n",
    "    print(results_df[['model', 'test_accuracy', 'f1_score', 'roc_auc']])\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nMejor modelo: {best_model} con accuracy={best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualización Comparativa de Todos los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "results_df_sorted = results_df.sort_values('test_accuracy', ascending=True)\n",
    "axes[0, 0].barh(results_df_sorted['model'], results_df_sorted['test_accuracy'])\n",
    "axes[0, 0].set_xlabel('Accuracy')\n",
    "axes[0, 0].set_title('Test Accuracy por Modelo')\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "results_df_sorted = results_df.sort_values('f1_score', ascending=True)\n",
    "axes[0, 1].barh(results_df_sorted['model'], results_df_sorted['f1_score'], color='orange')\n",
    "axes[0, 1].set_xlabel('F1 Score')\n",
    "axes[0, 1].set_title('F1 Score por Modelo')\n",
    "axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "results_df_sorted = results_df.sort_values('roc_auc', ascending=True)\n",
    "axes[1, 0].barh(results_df_sorted['model'], results_df_sorted['roc_auc'], color='green')\n",
    "axes[1, 0].set_xlabel('ROC-AUC')\n",
    "axes[1, 0].set_title('ROC-AUC por Modelo')\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "metrics_to_plot = ['precision', 'recall', 'f1_score']\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.25\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    axes[1, 1].bar(x + i*width, results_df[metric], width, label=metric)\n",
    "axes[1, 1].set_xlabel('Modelo')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Precision, Recall y F1-Score')\n",
    "axes[1, 1].set_xticks(x + width)\n",
    "axes[1, 1].set_xticklabels(results_df['model'], rotation=45, ha='right')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comprehensive_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualización guardada como 'comprehensive_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análisis de Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"cross_validation_analysis\"):\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    pipeline = create_pipeline(scaler_type='standard', model=model)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    mlflow.log_param(\"model\", \"RandomForest\")\n",
    "    mlflow.log_param(\"cv_folds\", 10)\n",
    "    mlflow.log_param(\"cv_strategy\", \"StratifiedKFold\")\n",
    "    \n",
    "    mlflow.log_metric(\"cv_mean\", cv_scores.mean())\n",
    "    mlflow.log_metric(\"cv_std\", cv_scores.std())\n",
    "    mlflow.log_metric(\"cv_min\", cv_scores.min())\n",
    "    mlflow.log_metric(\"cv_max\", cv_scores.max())\n",
    "    \n",
    "    for i, score in enumerate(cv_scores):\n",
    "        mlflow.log_metric(f\"cv_fold_{i+1}\", score)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, 11), cv_scores, marker='o', linestyle='-', linewidth=2, markersize=8)\n",
    "    plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', label=f'Mean: {cv_scores.mean():.4f}')\n",
    "    plt.fill_between(range(1, 11), \n",
    "                     cv_scores.mean() - cv_scores.std(),\n",
    "                     cv_scores.mean() + cv_scores.std(),\n",
    "                     alpha=0.2, color='red', label=f'±1 Std: {cv_scores.std():.4f}')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('10-Fold Cross-Validation Scores')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('cv_analysis.png')\n",
    "    mlflow.log_artifact('cv_analysis.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Cross-Validation Results:\")\n",
    "    print(f\"Mean Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    print(f\"Min Accuracy: {cv_scores.min():.4f}\")\n",
    "    print(f\"Max Accuracy: {cv_scores.max():.4f}\")\n",
    "    print(f\"\\nIndividual Fold Scores:\")\n",
    "    for i, score in enumerate(cv_scores, 1):\n",
    "        print(f\"Fold {i}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Búsqueda y Comparación de Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = mlflow.get_experiment_by_name(\"sklearn-classification-pipelines\")\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "print(f\"Total runs: {len(runs)}\")\n",
    "print(\"\\nTop 5 runs por test_accuracy:\")\n",
    "top_runs = runs.nlargest(5, 'metrics.test_accuracy')\n",
    "print(top_runs[['run_id', 'tags.mlflow.runName', 'metrics.test_accuracy', 'metrics.f1_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen del Módulo 1.2\n",
    "\n",
    "### Conceptos Clave:\n",
    "\n",
    "1. **Pipelines de scikit-learn**\n",
    "   - Encadenar preprocesamiento y modelo\n",
    "   - Logging completo del pipeline con MLflow\n",
    "   - Reproducibilidad garantizada\n",
    "\n",
    "2. **Nested Runs**\n",
    "   - Organizar experimentos jerárquicamente\n",
    "   - Parent run para comparaciones globales\n",
    "   - Child runs para configuraciones individuales\n",
    "\n",
    "3. **Comparación de Modelos**\n",
    "   - Tracking sistemático de múltiples algoritmos\n",
    "   - Métricas estandarizadas\n",
    "   - Visualizaciones automáticas\n",
    "\n",
    "4. **Cross-Validation con MLflow**\n",
    "   - Logging de scores por fold\n",
    "   - Análisis de variabilidad\n",
    "\n",
    "### Siguiente Paso:\n",
    "En el siguiente notebook trabajaremos con regresión y técnicas avanzadas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
