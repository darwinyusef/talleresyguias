{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo 1.3: Regresión Avanzada con MLflow\n",
    "\n",
    "## Teoría: Regresión en Machine Learning\n",
    "\n",
    "### ¿Qué es la regresión?\n",
    "La regresión es una técnica de aprendizaje supervisado que predice valores continuos (a diferencia de la clasificación que predice categorías discretas).\n",
    "\n",
    "### Tipos de regresión:\n",
    "1. **Regresión Lineal**: Asume relación lineal entre variables\n",
    "   - Simple: una variable independiente\n",
    "   - Múltiple: varias variables independientes\n",
    "   \n",
    "2. **Regresión Polinomial**: Captura relaciones no lineales\n",
    "\n",
    "3. **Regresión Ridge (L2)**: Añade regularización para prevenir overfitting\n",
    "   - Penaliza coeficientes grandes\n",
    "   - Útil cuando hay multicolinealidad\n",
    "\n",
    "4. **Regresión Lasso (L1)**: Regularización que puede eliminar features\n",
    "   - Feature selection automática\n",
    "   - Produce modelos sparse\n",
    "\n",
    "5. **Elastic Net**: Combina L1 y L2\n",
    "\n",
    "6. **Árboles de decisión y ensembles**: Random Forest, Gradient Boosting\n",
    "\n",
    "### Métricas de evaluación:\n",
    "- **MAE (Mean Absolute Error)**: Error promedio absoluto\n",
    "- **MSE (Mean Squared Error)**: Error cuadrático medio (penaliza más los errores grandes)\n",
    "- **RMSE (Root Mean Squared Error)**: Raíz del MSE (mismas unidades que el target)\n",
    "- **R² Score**: Proporción de varianza explicada (0-1, siendo 1 perfecto)\n",
    "- **MAPE (Mean Absolute Percentage Error)**: Error porcentual\n",
    "\n",
    "## Objetivos\n",
    "- Implementar múltiples algoritmos de regresión\n",
    "- Tracking completo con MLflow\n",
    "- Feature engineering y selección\n",
    "- Análisis de residuos\n",
    "- Regularización y tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Ridge, Lasso, ElasticNet, \n",
    "    HuberRegressor, RANSACRegressor\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor,\n",
    "    AdaBoostRegressor, ExtraTreesRegressor\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"sklearn-regression-advanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset: California Housing\n",
    "\n",
    "Dataset de precios de viviendas en California con 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california = fetch_california_housing()\n",
    "X = california.data\n",
    "y = california.target\n",
    "\n",
    "df = pd.DataFrame(X, columns=california.feature_names)\n",
    "df['MedHouseVal'] = y\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFeatures:\")\n",
    "for i, feature in enumerate(california.feature_names):\n",
    "    print(f\"{i+1}. {feature}\")\n",
    "\n",
    "print(\"\\nTarget variable (MedHouseVal): Median house value in $100,000s\")\n",
    "print(f\"Mean: ${y.mean()*100000:.2f}\")\n",
    "print(f\"Median: ${np.median(y)*100000:.2f}\")\n",
    "print(f\"Min: ${y.min()*100000:.2f}\")\n",
    "print(f\"Max: ${y.max()*100000:.2f}\")\n",
    "\n",
    "print(\"\\nStatistical summary:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(california.feature_names):\n",
    "    axes[i].scatter(df[col], df['MedHouseVal'], alpha=0.3, s=1)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('MedHouseVal')\n",
    "    axes[i].set_title(f'{col} vs MedHouseVal')\n",
    "    \n",
    "    corr = np.corrcoef(df[col], df['MedHouseVal'])[0, 1]\n",
    "    axes[i].text(0.05, 0.95, f'Corr: {corr:.3f}', \n",
    "                transform=axes[i].transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "axes[8].hist(df['MedHouseVal'], bins=50, edgecolor='black')\n",
    "axes[8].set_xlabel('MedHouseVal')\n",
    "axes[8].set_ylabel('Frequency')\n",
    "axes[8].set_title('Distribution of Target Variable')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eda_california.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlaciones con el target (MedHouseVal):\")\n",
    "print(correlation_matrix['MedHouseVal'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Función de Evaluación Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression_model(model, X_train, y_train, X_test, y_test, model_name=\"Model\"):\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'train_mae': mean_absolute_error(y_train, y_pred_train),\n",
    "        'train_mse': mean_squared_error(y_train, y_pred_train),\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        'train_r2': r2_score(y_train, y_pred_train),\n",
    "        \n",
    "        'test_mae': mean_absolute_error(y_test, y_pred_test),\n",
    "        'test_mse': mean_squared_error(y_test, y_pred_test),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'test_r2': r2_score(y_test, y_pred_test),\n",
    "        'test_mape': mean_absolute_percentage_error(y_test, y_pred_test)\n",
    "    }\n",
    "    \n",
    "    residuals = y_test - y_pred_test\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    axes[0, 0].scatter(y_test, y_pred_test, alpha=0.5, s=10)\n",
    "    axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    axes[0, 0].set_xlabel('Actual Values')\n",
    "    axes[0, 0].set_ylabel('Predicted Values')\n",
    "    axes[0, 0].set_title(f'Actual vs Predicted - {model_name}')\n",
    "    axes[0, 0].text(0.05, 0.95, f\"R² = {metrics['test_r2']:.4f}\", \n",
    "                   transform=axes[0, 0].transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    axes[0, 1].scatter(y_pred_test, residuals, alpha=0.5, s=10)\n",
    "    axes[0, 1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    axes[0, 1].set_xlabel('Predicted Values')\n",
    "    axes[0, 1].set_ylabel('Residuals')\n",
    "    axes[0, 1].set_title('Residual Plot')\n",
    "    \n",
    "    axes[1, 0].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Residuals')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Distribution of Residuals')\n",
    "    axes[1, 0].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "    \n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1, 1])\n",
    "    axes[1, 1].set_title('Q-Q Plot')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_filename = f'evaluation_{model_name.replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(plot_filename, dpi=150)\n",
    "    plt.close()\n",
    "    \n",
    "    return metrics, residuals, plot_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimento 1: Regresión Lineal Básica\n",
    "\n",
    "### Teoría: Regresión Lineal\n",
    "La regresión lineal asume que la relación entre las variables independientes (X) y la dependiente (y) es lineal:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon$$\n",
    "\n",
    "Donde:\n",
    "- $\\beta_0$ es el intercepto\n",
    "- $\\beta_i$ son los coeficientes\n",
    "- $\\epsilon$ es el error\n",
    "\n",
    "El objetivo es minimizar la suma de errores cuadráticos (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"linear_regression_baseline\"):\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "    mlflow.log_param(\"scaler\", \"StandardScaler\")\n",
    "    mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "    \n",
    "    metrics, residuals, plot_file = evaluate_regression_model(\n",
    "        model, X_train_scaled, y_train, X_test_scaled, y_test, \"LinearRegression\"\n",
    "    )\n",
    "    \n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "    \n",
    "    mlflow.log_artifact(plot_file)\n",
    "    \n",
    "    coefficients_df = pd.DataFrame({\n",
    "        'feature': california.feature_names,\n",
    "        'coefficient': model.coef_\n",
    "    }).sort_values('coefficient', ascending=False)\n",
    "    coefficients_df.to_csv('linear_reg_coefficients.csv', index=False)\n",
    "    mlflow.log_artifact('linear_reg_coefficients.csv')\n",
    "    \n",
    "    mlflow.sklearn.log_model(model, \"linear_regression_model\")\n",
    "    \n",
    "    mlflow.set_tag(\"model_family\", \"linear\")\n",
    "    mlflow.set_tag(\"dataset\", \"california_housing\")\n",
    "    \n",
    "    print(\"Linear Regression Results:\")\n",
    "    print(f\"Train R²: {metrics['train_r2']:.4f}\")\n",
    "    print(f\"Test R²: {metrics['test_r2']:.4f}\")\n",
    "    print(f\"Test RMSE: ${metrics['test_rmse']*100000:.2f}\")\n",
    "    print(f\"Test MAE: ${metrics['test_mae']*100000:.2f}\")\n",
    "    print(f\"Test MAPE: {metrics['test_mape']*100:.2f}%\")\n",
    "    \n",
    "    print(\"\\nFeature Importance (by coefficient magnitude):\")\n",
    "    print(coefficients_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experimento 2: Regularización (Ridge, Lasso, ElasticNet)\n",
    "\n",
    "### Teoría: Regularización\n",
    "\n",
    "**Ridge (L2 Regularization)**:\n",
    "$$\\text{Cost} = MSE + \\alpha \\sum_{i=1}^{n} \\beta_i^2$$\n",
    "\n",
    "- Penaliza coeficientes grandes\n",
    "- No elimina features (coeficientes → 0 pero nunca = 0)\n",
    "- Útil cuando hay multicolinealidad\n",
    "\n",
    "**Lasso (L1 Regularization)**:\n",
    "$$\\text{Cost} = MSE + \\alpha \\sum_{i=1}^{n} |\\beta_i|$$\n",
    "\n",
    "- Puede eliminar features (coeficientes = 0)\n",
    "- Feature selection automática\n",
    "- Modelos más interpretables\n",
    "\n",
    "**ElasticNet**: Combina L1 y L2\n",
    "$$\\text{Cost} = MSE + \\alpha_1 \\sum_{i=1}^{n} |\\beta_i| + \\alpha_2 \\sum_{i=1}^{n} \\beta_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_models = [\n",
    "    {'name': 'Ridge_alpha_0.1', 'model': Ridge(alpha=0.1, random_state=42)},\n",
    "    {'name': 'Ridge_alpha_1.0', 'model': Ridge(alpha=1.0, random_state=42)},\n",
    "    {'name': 'Ridge_alpha_10', 'model': Ridge(alpha=10.0, random_state=42)},\n",
    "    {'name': 'Lasso_alpha_0.01', 'model': Lasso(alpha=0.01, random_state=42)},\n",
    "    {'name': 'Lasso_alpha_0.1', 'model': Lasso(alpha=0.1, random_state=42)},\n",
    "    {'name': 'ElasticNet_alpha_0.1', 'model': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)},\n",
    "]\n",
    "\n",
    "with mlflow.start_run(run_name=\"regularization_comparison\") as parent_run:\n",
    "    \n",
    "    mlflow.set_tag(\"experiment_type\", \"regularization\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for config in regularization_models:\n",
    "        with mlflow.start_run(run_name=config['name'], nested=True):\n",
    "            \n",
    "            model = config['model']\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            mlflow.log_param(\"model_type\", config['name'].split('_')[0])\n",
    "            \n",
    "            if hasattr(model, 'alpha'):\n",
    "                mlflow.log_param(\"alpha\", model.alpha)\n",
    "            if hasattr(model, 'l1_ratio'):\n",
    "                mlflow.log_param(\"l1_ratio\", model.l1_ratio)\n",
    "            \n",
    "            metrics, residuals, plot_file = evaluate_regression_model(\n",
    "                model, X_train_scaled, y_train, X_test_scaled, y_test, config['name']\n",
    "            )\n",
    "            \n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(metric_name, metric_value)\n",
    "            \n",
    "            mlflow.log_artifact(plot_file)\n",
    "            \n",
    "            n_features_used = np.sum(model.coef_ != 0)\n",
    "            mlflow.log_metric(\"n_features_used\", n_features_used)\n",
    "            \n",
    "            mlflow.sklearn.log_model(model, f\"model_{config['name']}\")\n",
    "            \n",
    "            results.append({\n",
    "                'model': config['name'],\n",
    "                'test_r2': metrics['test_r2'],\n",
    "                'test_rmse': metrics['test_rmse'],\n",
    "                'test_mae': metrics['test_mae'],\n",
    "                'n_features_used': n_features_used\n",
    "            })\n",
    "            \n",
    "            print(f\"{config['name']}: R²={metrics['test_r2']:.4f}, \"\n",
    "                  f\"RMSE={metrics['test_rmse']:.4f}, Features={n_features_used}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('regularization_comparison.csv', index=False)\n",
    "    mlflow.log_artifact('regularization_comparison.csv')\n",
    "    \n",
    "    print(\"\\nRegularization Comparison:\")\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experimento 3: Tree-Based Models\n",
    "\n",
    "### Teoría: Modelos basados en árboles\n",
    "\n",
    "**Random Forest**:\n",
    "- Ensemble de árboles de decisión\n",
    "- Bootstrap aggregating (bagging)\n",
    "- Reduce varianza\n",
    "- Robusto a outliers\n",
    "\n",
    "**Gradient Boosting**:\n",
    "- Ensemble secuencial\n",
    "- Cada árbol corrige errores del anterior\n",
    "- Típicamente mejor performance que RF\n",
    "- Más propenso a overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_models = [\n",
    "    {'name': 'RandomForest_50', 'model': RandomForestRegressor(n_estimators=50, random_state=42)},\n",
    "    {'name': 'RandomForest_100', 'model': RandomForestRegressor(n_estimators=100, random_state=42)},\n",
    "    {'name': 'GradientBoosting', 'model': GradientBoostingRegressor(n_estimators=100, random_state=42)},\n",
    "    {'name': 'ExtraTrees', 'model': ExtraTreesRegressor(n_estimators=100, random_state=42)},\n",
    "]\n",
    "\n",
    "with mlflow.start_run(run_name=\"tree_models_comparison\") as parent_run:\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for config in tree_models:\n",
    "        with mlflow.start_run(run_name=config['name'], nested=True):\n",
    "            \n",
    "            model = config['model']\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            mlflow.log_param(\"model_type\", config['name'])\n",
    "            if hasattr(model, 'n_estimators'):\n",
    "                mlflow.log_param(\"n_estimators\", model.n_estimators)\n",
    "            \n",
    "            metrics, residuals, plot_file = evaluate_regression_model(\n",
    "                model, X_train, y_train, X_test, y_test, config['name']\n",
    "            )\n",
    "            \n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(metric_name, metric_value)\n",
    "            \n",
    "            mlflow.log_artifact(plot_file)\n",
    "            \n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'feature': california.feature_names,\n",
    "                    'importance': model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "                plt.xlabel('Importance')\n",
    "                plt.title(f'Feature Importance - {config[\"name\"]}')\n",
    "                plt.tight_layout()\n",
    "                importance_plot = f'importance_{config[\"name\"]}.png'\n",
    "                plt.savefig(importance_plot)\n",
    "                mlflow.log_artifact(importance_plot)\n",
    "                plt.close()\n",
    "            \n",
    "            mlflow.sklearn.log_model(model, f\"model_{config['name']}\")\n",
    "            \n",
    "            results.append({\n",
    "                'model': config['name'],\n",
    "                'test_r2': metrics['test_r2'],\n",
    "                'test_rmse': metrics['test_rmse'],\n",
    "                'test_mae': metrics['test_mae']\n",
    "            })\n",
    "            \n",
    "            print(f\"{config['name']}: R²={metrics['test_r2']:.4f}, RMSE={metrics['test_rmse']:.4f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nTree Models Comparison:\")\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparación Global de Todos los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = mlflow.get_experiment_by_name(\"sklearn-regression-advanced\")\n",
    "all_runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "runs_with_metrics = all_runs.dropna(subset=['metrics.test_r2'])\n",
    "top_runs = runs_with_metrics.nlargest(10, 'metrics.test_r2')\n",
    "\n",
    "print(\"Top 10 Models por R² Score:\")\n",
    "print(top_runs[['tags.mlflow.runName', 'metrics.test_r2', 'metrics.test_rmse', 'metrics.test_mae']])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(top_runs)), top_runs['metrics.test_r2'])\n",
    "plt.yticks(range(len(top_runs)), top_runs['tags.mlflow.runName'])\n",
    "plt.xlabel('R² Score')\n",
    "plt.title('Top 10 Models - R² Score Comparison')\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_model_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen del Módulo 1.3\n",
    "\n",
    "### Conceptos Clave:\n",
    "\n",
    "1. **Regresión Lineal y Regularización**\n",
    "   - Ridge: reduce magnitud de coeficientes\n",
    "   - Lasso: elimina features\n",
    "   - ElasticNet: lo mejor de ambos mundos\n",
    "\n",
    "2. **Modelos basados en árboles**\n",
    "   - Random Forest: bagging, reduce varianza\n",
    "   - Gradient Boosting: boosting, corrige errores\n",
    "\n",
    "3. **Métricas de Regresión**\n",
    "   - R²: proporción de varianza explicada\n",
    "   - RMSE: error en unidades del target\n",
    "   - MAE: error promedio absoluto\n",
    "   - MAPE: error porcentual\n",
    "\n",
    "4. **Análisis de Residuos**\n",
    "   - Scatter plots\n",
    "   - Histogramas\n",
    "   - Q-Q plots\n",
    "\n",
    "### Mejores Prácticas con MLflow:\n",
    "- Nested runs para comparaciones\n",
    "- Logging completo de métricas\n",
    "- Visualizaciones automáticas\n",
    "- Feature importance tracking\n",
    "\n",
    "### Próximo Módulo:\n",
    "Deep Learning con TensorFlow/Keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
