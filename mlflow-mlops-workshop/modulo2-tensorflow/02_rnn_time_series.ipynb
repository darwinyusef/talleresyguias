{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo 2.2: RNN/LSTM para Series Temporales con MLflow\n",
    "\n",
    "## Teoría: Redes Neuronales Recurrentes (RNN)\n",
    "\n",
    "### ¿Qué son las RNNs?\n",
    "Las RNNs son arquitecturas diseñadas para procesar **secuencias** de datos, donde el orden importa:\n",
    "- Series temporales\n",
    "- Texto y lenguaje natural\n",
    "- Audio y video\n",
    "- Datos secuenciales en general\n",
    "\n",
    "### Arquitecturas de RNN:\n",
    "\n",
    "#### 1. **RNN Vanilla**\n",
    "```\n",
    "h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)\n",
    "y_t = W_hy * h_t + b_y\n",
    "```\n",
    "- **Problema**: Vanishing/Exploding gradients\n",
    "- No captura dependencias a largo plazo\n",
    "\n",
    "#### 2. **LSTM (Long Short-Term Memory)**\n",
    "Soluciona el problema de gradientes con:\n",
    "- **Forget Gate**: Qué olvidar del estado previo\n",
    "- **Input Gate**: Qué información nueva agregar\n",
    "- **Output Gate**: Qué output generar\n",
    "- **Cell State**: Memoria a largo plazo\n",
    "\n",
    "```\n",
    "f_t = σ(W_f · [h_{t-1}, x_t] + b_f)     # Forget gate\n",
    "i_t = σ(W_i · [h_{t-1}, x_t] + b_i)     # Input gate\n",
    "C̃_t = tanh(W_C · [h_{t-1}, x_t] + b_C)  # Candidate values\n",
    "C_t = f_t * C_{t-1} + i_t * C̃_t         # New cell state\n",
    "o_t = σ(W_o · [h_{t-1}, x_t] + b_o)     # Output gate\n",
    "h_t = o_t * tanh(C_t)                   # New hidden state\n",
    "```\n",
    "\n",
    "**Ventajas de LSTM**:\n",
    "- Captura dependencias a largo plazo\n",
    "- Evita vanishing gradients\n",
    "- Memoria selectiva\n",
    "\n",
    "#### 3. **GRU (Gated Recurrent Unit)**\n",
    "Versión simplificada de LSTM:\n",
    "- Menos parámetros\n",
    "- Más rápido de entrenar\n",
    "- Rendimiento similar en muchos casos\n",
    "\n",
    "### Bidirectional RNN\n",
    "Procesa la secuencia en ambas direcciones:\n",
    "```\n",
    "Forward:  x_1 → x_2 → x_3 → ... → x_n\n",
    "Backward: x_n → ... → x_3 → x_2 → x_1\n",
    "```\n",
    "Útil cuando el contexto futuro también importa.\n",
    "\n",
    "## Objetivos\n",
    "- Construir RNN/LSTM para series temporales\n",
    "- Forecasting de múltiples pasos\n",
    "- Tracking con MLflow\n",
    "- Arquitecturas bidireccionales\n",
    "- Stacked LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"tensorflow-rnn-timeseries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generación de Serie Temporal Sintética\n",
    "\n",
    "Vamos a generar una serie temporal con múltiples componentes:\n",
    "- Tendencia\n",
    "- Estacionalidad\n",
    "- Ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_timeseries(n_samples=2000, seed=42):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    time = np.arange(n_samples)\n",
    "    \n",
    "    trend = 0.01 * time + 10\n",
    "    \n",
    "    seasonal_1 = 5 * np.sin(2 * np.pi * time / 50)\n",
    "    seasonal_2 = 3 * np.cos(2 * np.pi * time / 100)\n",
    "    \n",
    "    noise = np.random.normal(0, 1, n_samples)\n",
    "    \n",
    "    series = trend + seasonal_1 + seasonal_2 + noise\n",
    "    \n",
    "    return time, series\n",
    "\n",
    "time, series = generate_synthetic_timeseries()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(time, series, linewidth=0.8)\n",
    "plt.title('Synthetic Time Series')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('timeseries_full.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Series length: {len(series)}\")\n",
    "print(f\"Mean: {series.mean():.2f}\")\n",
    "print(f\"Std: {series.std():.2f}\")\n",
    "print(f\"Min: {series.min():.2f}\")\n",
    "print(f\"Max: {series.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparación de Datos para RNN\n",
    "\n",
    "Transformamos la serie temporal en secuencias supervisadas:\n",
    "- **X**: Ventana de `window_size` valores pasados\n",
    "- **y**: Siguiente valor a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, window_size, forecast_horizon=1):\n",
    "    \"\"\"\n",
    "    Crea secuencias para entrenamiento de RNN\n",
    "    \n",
    "    Args:\n",
    "        data: Serie temporal 1D\n",
    "        window_size: Tamaño de la ventana de entrada\n",
    "        forecast_horizon: Cuántos pasos adelante predecir\n",
    "    \n",
    "    Returns:\n",
    "        X: Secuencias de entrada (samples, window_size, 1)\n",
    "        y: Targets (samples, forecast_horizon)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - window_size - forecast_horizon + 1):\n",
    "        X.append(data[i:i + window_size])\n",
    "        \n",
    "        if forecast_horizon == 1:\n",
    "            y.append(data[i + window_size])\n",
    "        else:\n",
    "            y.append(data[i + window_size:i + window_size + forecast_horizon])\n",
    "    \n",
    "    X = np.array(X).reshape(-1, window_size, 1)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "series_scaled = scaler.fit_transform(series.reshape(-1, 1)).flatten()\n",
    "\n",
    "window_size = 50\n",
    "forecast_horizon = 1\n",
    "\n",
    "X, y = create_sequences(series_scaled, window_size, forecast_horizon)\n",
    "\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"Input window (first 5 values): {X_train[0, :5, 0]}\")\n",
    "print(f\"Target: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelo Simple: LSTM Básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_lstm(window_size, units=50):\n",
    "    model = models.Sequential([\n",
    "        layers.LSTM(units, input_shape=(window_size, 1)),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = create_simple_lstm(window_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"simple_lstm\") as run:\n",
    "    \n",
    "    model = create_simple_lstm(window_size, units=50)\n",
    "    \n",
    "    lr = 0.001\n",
    "    epochs = 50\n",
    "    batch_size = 32\n",
    "    \n",
    "    mlflow.log_param(\"model_type\", \"SimpleLSTM\")\n",
    "    mlflow.log_param(\"lstm_units\", 50)\n",
    "    mlflow.log_param(\"window_size\", window_size)\n",
    "    mlflow.log_param(\"learning_rate\", lr)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"total_parameters\", model.count_params())\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    y_pred_train = model.predict(X_train).flatten()\n",
    "    y_pred_test = model.predict(X_test).flatten()\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    \n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    mlflow.log_metric(\"train_mse\", train_mse)\n",
    "    mlflow.log_metric(\"train_mae\", train_mae)\n",
    "    mlflow.log_metric(\"train_r2\", train_r2)\n",
    "    mlflow.log_metric(\"test_mse\", test_mse)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_r2\", test_r2)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    axes[0, 0].plot(history.history['loss'], label='Train Loss')\n",
    "    axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('MSE')\n",
    "    axes[0, 0].set_title('Training History')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].plot(y_test[:200], label='Actual', alpha=0.7)\n",
    "    axes[0, 1].plot(y_pred_test[:200], label='Predicted', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Time')\n",
    "    axes[0, 1].set_ylabel('Value (Normalized)')\n",
    "    axes[0, 1].set_title('Predictions vs Actual (First 200 points)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].scatter(y_test, y_pred_test, alpha=0.5, s=10)\n",
    "    axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    axes[1, 0].set_xlabel('Actual')\n",
    "    axes[1, 0].set_ylabel('Predicted')\n",
    "    axes[1, 0].set_title(f'Actual vs Predicted (R²={test_r2:.4f})')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    residuals = y_test - y_pred_test\n",
    "    axes[1, 1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Residual')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Residuals Distribution')\n",
    "    axes[1, 1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lstm_evaluation.png', dpi=150)\n",
    "    mlflow.log_artifact('lstm_evaluation.png')\n",
    "    plt.show()\n",
    "    \n",
    "    mlflow.keras.log_model(model, \"lstm_model\")\n",
    "    \n",
    "    mlflow.set_tag(\"architecture\", \"LSTM\")\n",
    "    mlflow.set_tag(\"task\", \"time_series_forecasting\")\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"MSE: {test_mse:.6f}\")\n",
    "    print(f\"MAE: {test_mae:.6f}\")\n",
    "    print(f\"R²: {test_r2:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelo Avanzado: Stacked LSTM Bidireccional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacked_bidirectional_lstm(window_size):\n",
    "    model = models.Sequential([\n",
    "        layers.Bidirectional(\n",
    "            layers.LSTM(64, return_sequences=True),\n",
    "            input_shape=(window_size, 1)\n",
    "        ),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Bidirectional(\n",
    "            layers.LSTM(32, return_sequences=True)\n",
    "        ),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Bidirectional(\n",
    "            layers.LSTM(16)\n",
    "        ),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model_advanced = create_stacked_bidirectional_lstm(window_size)\n",
    "model_advanced.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"stacked_bidirectional_lstm\") as run:\n",
    "    \n",
    "    model = create_stacked_bidirectional_lstm(window_size)\n",
    "    \n",
    "    lr = 0.001\n",
    "    epochs = 50\n",
    "    batch_size = 32\n",
    "    \n",
    "    mlflow.log_param(\"model_type\", \"StackedBidirectionalLSTM\")\n",
    "    mlflow.log_param(\"num_lstm_layers\", 3)\n",
    "    mlflow.log_param(\"bidirectional\", True)\n",
    "    mlflow.log_param(\"window_size\", window_size)\n",
    "    mlflow.log_param(\"learning_rate\", lr)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"total_parameters\", model.count_params())\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    y_pred_test = model.predict(X_test).flatten()\n",
    "    \n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    mlflow.log_metric(\"test_mse\", test_mse)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_r2\", test_r2)\n",
    "    \n",
    "    mlflow.keras.log_model(model, \"stacked_bidirectional_lstm_model\")\n",
    "    \n",
    "    print(f\"\\nStacked Bidirectional LSTM Results:\")\n",
    "    print(f\"Test R²: {test_r2:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Step Forecasting\n",
    "\n",
    "Predecir múltiples pasos hacia el futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 10\n",
    "X_multi, y_multi = create_sequences(series_scaled, window_size, forecast_horizon)\n",
    "\n",
    "train_size = int(0.8 * len(X_multi))\n",
    "X_train_multi, X_test_multi = X_multi[:train_size], X_multi[train_size:]\n",
    "y_train_multi, y_test_multi = y_multi[:train_size], y_multi[train_size:]\n",
    "\n",
    "print(f\"Multi-step forecasting:\")\n",
    "print(f\"X_train shape: {X_train_multi.shape}\")\n",
    "print(f\"y_train shape: {y_train_multi.shape}\")\n",
    "print(f\"Forecast horizon: {forecast_horizon} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multistep_lstm(window_size, forecast_horizon):\n",
    "    model = models.Sequential([\n",
    "        layers.LSTM(100, return_sequences=True, input_shape=(window_size, 1)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.LSTM(50),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(50, activation='relu'),\n",
    "        layers.Dense(forecast_horizon)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "with mlflow.start_run(run_name=\"multistep_lstm\") as run:\n",
    "    \n",
    "    model = create_multistep_lstm(window_size, forecast_horizon)\n",
    "    \n",
    "    mlflow.log_param(\"model_type\", \"MultistepLSTM\")\n",
    "    mlflow.log_param(\"forecast_horizon\", forecast_horizon)\n",
    "    mlflow.log_param(\"window_size\", window_size)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_multi, y_train_multi,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    y_pred_multi = model.predict(X_test_multi)\n",
    "    \n",
    "    test_mse = mean_squared_error(y_test_multi, y_pred_multi)\n",
    "    test_mae = mean_absolute_error(y_test_multi, y_pred_multi)\n",
    "    \n",
    "    mlflow.log_metric(\"test_mse\", test_mse)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "    \n",
    "    for i in range(min(5, len(y_test_multi))):\n",
    "        axes[0].plot(range(forecast_horizon), y_test_multi[i], 'o-', alpha=0.6, label=f'Actual {i+1}')\n",
    "        axes[0].plot(range(forecast_horizon), y_pred_multi[i], 'x--', alpha=0.6, label=f'Pred {i+1}')\n",
    "    axes[0].set_xlabel('Forecast Step')\n",
    "    axes[0].set_ylabel('Value')\n",
    "    axes[0].set_title(f'{forecast_horizon}-Step Ahead Forecasts')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    step_errors = []\n",
    "    for step in range(forecast_horizon):\n",
    "        step_mae = mean_absolute_error(y_test_multi[:, step], y_pred_multi[:, step])\n",
    "        step_errors.append(step_mae)\n",
    "    \n",
    "    axes[1].plot(range(1, forecast_horizon + 1), step_errors, 'o-', linewidth=2)\n",
    "    axes[1].set_xlabel('Forecast Horizon (steps ahead)')\n",
    "    axes[1].set_ylabel('MAE')\n",
    "    axes[1].set_title('Error by Forecast Horizon')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('multistep_forecast.png', dpi=150)\n",
    "    mlflow.log_artifact('multistep_forecast.png')\n",
    "    plt.show()\n",
    "    \n",
    "    mlflow.keras.log_model(model, \"multistep_lstm_model\")\n",
    "    \n",
    "    print(f\"Multi-step forecast MSE: {test_mse:.6f}\")\n",
    "    print(f\"Multi-step forecast MAE: {test_mae:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparación de Modelos RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = mlflow.get_experiment_by_name(\"tensorflow-rnn-timeseries\")\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "print(\"Comparación de modelos RNN/LSTM:\")\n",
    "comparison = runs[[\n",
    "    'tags.mlflow.runName',\n",
    "    'metrics.test_r2',\n",
    "    'metrics.test_mse',\n",
    "    'metrics.test_mae',\n",
    "    'params.total_parameters'\n",
    "]].sort_values('metrics.test_r2', ascending=False)\n",
    "\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen del Módulo 2.2\n",
    "\n",
    "### Conceptos Clave:\n",
    "\n",
    "1. **RNN y LSTM**\n",
    "   - RNN para datos secuenciales\n",
    "   - LSTM soluciona vanishing gradients\n",
    "   - GRU como alternativa más simple\n",
    "\n",
    "2. **Arquitecturas Avanzadas**\n",
    "   - Stacked LSTMs: múltiples capas\n",
    "   - Bidirectional: procesa en ambas direcciones\n",
    "   - Combinación de ambas\n",
    "\n",
    "3. **Forecasting**\n",
    "   - Single-step: predecir siguiente valor\n",
    "   - Multi-step: predecir múltiples pasos\n",
    "   - Error aumenta con horizonte de predicción\n",
    "\n",
    "4. **Preparación de Datos**\n",
    "   - Crear secuencias con ventana deslizante\n",
    "   - Normalización es crucial\n",
    "   - Train/test split temporal (no aleatorio)\n",
    "\n",
    "### MLflow para Series Temporales:\n",
    "- Log de window_size y forecast_horizon\n",
    "- Tracking de métricas por horizonte de predicción\n",
    "- Visualización de predicciones\n",
    "- Comparación de arquitecturas\n",
    "\n",
    "### Próximo Módulo:\n",
    "PyTorch con Transfer Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
