# ğŸ“š GuÃ­a MLOps - Tomo 2: Docker y Servicios BÃ¡sicos

> **Nivel:** Principiante-Intermedio
> **DuraciÃ³n estimada:** 3-4 horas
> **Prerequisito:** Tomo 1 completado

---

## ğŸ¯ Objetivos de este Tomo

Al finalizar este tomo podrÃ¡s:
- âœ… Entender cÃ³mo funcionan los contenedores Docker
- âœ… Usar PyTorch Service para hacer predicciones
- âœ… Registrar experimentos en MLflow
- âœ… Trabajar con FastAPI y APIs REST
- âœ… Leer y entender logs
- âœ… Monitorear mÃ©tricas con Prometheus

---

## ğŸ“– CapÃ­tulo 1: Fundamentos de Docker

### 1.1 Â¿QuÃ© es un Contenedor?

**AnalogÃ­a:** Un contenedor es como una caja de LEGO completa

```
ğŸ  Tu Computadora (Host)
â””â”€â”€ ğŸ“¦ Contenedor 1: PyTorch Service
    â”œâ”€â”€ Ubuntu Linux
    â”œâ”€â”€ Python 3.11
    â”œâ”€â”€ PyTorch 2.1
    â”œâ”€â”€ Tu cÃ³digo
    â””â”€â”€ SOLO lo necesario

â””â”€â”€ ğŸ“¦ Contenedor 2: PostgreSQL
    â”œâ”€â”€ Linux
    â”œâ”€â”€ PostgreSQL 16
    â””â”€â”€ Datos de MLflow
```

**Ventajas:**
- âœ… **Aislamiento**: Cada contenedor es independiente
- âœ… **Portabilidad**: Funciona igual en cualquier lugar
- âœ… **Ligero**: Comparte el kernel del sistema
- âœ… **RÃ¡pido**: Inicia en segundos

### 1.2 Imagen vs Contenedor

**Imagen** = Receta ğŸ“„
- Template read-only
- Se construye una vez
- Se puede compartir

**Contenedor** = Plato cocinado ğŸ½ï¸
- Instancia de una imagen
- Se ejecuta y puede cambiar
- Temporal (se puede crear/destruir)

```bash
# Ver imÃ¡genes disponibles
docker images

# Ver contenedores corriendo
docker ps

# Ver TODOS los contenedores (incluso parados)
docker ps -a
```

### 1.3 AnatomÃ­a de un Dockerfile

Nuestro `Dockerfile.pytorch`:

```dockerfile
# ============================================
# PASO 1: Imagen base
# ============================================
FROM pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime
# â†‘ Partimos de una imagen que ya tiene PyTorch

# ============================================
# PASO 2: Metadata
# ============================================
LABEL maintainer="mlops@example.com"
LABEL description="PyTorch ML Service"
# â†‘ InformaciÃ³n sobre la imagen

# ============================================
# PASO 3: Variables de entorno
# ============================================
ENV PYTHONUNBUFFERED=1
# â†‘ Configuraciones del ambiente

# ============================================
# PASO 4: Instalar dependencias del sistema
# ============================================
RUN apt-get update && apt-get install -y \
    curl \
    git
# â†‘ Instala software necesario

# ============================================
# PASO 5: Copiar cÃ³digo
# ============================================
COPY ./src ./src
COPY requirements.txt .
# â†‘ Copia archivos de tu mÃ¡quina al contenedor

# ============================================
# PASO 6: Instalar dependencias Python
# ============================================
RUN pip install -r requirements.txt
# â†‘ Instala librerÃ­as de Python

# ============================================
# PASO 7: Puerto
# ============================================
EXPOSE 8000
# â†‘ Declara quÃ© puerto usa el servicio

# ============================================
# PASO 8: Comando de inicio
# ============================================
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0"]
# â†‘ QuÃ© ejecutar cuando inicia el contenedor
```

### 1.4 Docker Compose - OrquestaciÃ³n

**docker-compose.yml** define TODOS los servicios:

```yaml
services:
  # Cada servicio es un contenedor

  pytorch-service:
    build: .                    # Construir desde Dockerfile
    ports:
      - "8000:8000"            # Puerto host:container
    environment:
      - SERVICE_NAME=pytorch   # Variables de entorno
    volumes:
      - ./src:/app/src         # Montar carpetas
    depends_on:
      - rabbitmq               # Esperar a RabbitMQ
      - redis
    networks:
      - mlops-network          # Red compartida
```

**Â¿Por quÃ© Docker Compose?**
- Un solo comando para manejar 11 servicios
- Define dependencias entre servicios
- Crea redes automÃ¡ticamente
- Gestiona volÃºmenes de datos

---

## ğŸ“– CapÃ­tulo 2: PyTorch Service en Profundidad

### 2.1 Arquitectura del Servicio

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      PyTorch Service (FastAPI)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Puerto 8000
    â”‚
    â”œâ”€â”€ GET /              â†’ Info del servicio
    â”œâ”€â”€ GET /health        â†’ Health check
    â”œâ”€â”€ POST /predict      â†’ Hacer predicciÃ³n
    â””â”€â”€ GET /metrics       â†’ MÃ©tricas Prometheus
```

### 2.2 Explorar el CÃ³digo

**Abrir el archivo:** `src/main.py`

```python
# ============================================
# IMPORTACIONES
# ============================================
from fastapi import FastAPI
# â†‘ Framework web moderno y rÃ¡pido

import uvicorn
# â†‘ Servidor ASGI para FastAPI

# ============================================
# CREAR APLICACIÃ“N
# ============================================
app = FastAPI(
    title="MLOps Platform",
    version="1.0.0",
)
# â†‘ Instancia de FastAPI con metadata

# ============================================
# ENDPOINT RAÃZ
# ============================================
@app.get("/")
async def root():
    """
    Endpoint principal
    Responde con informaciÃ³n del servicio
    """
    return {
        "service": "pytorch-service",
        "version": "1.0.0",
        "status": "running",
    }

# ============================================
# HEALTH CHECK
# ============================================
@app.get("/health")
async def health():
    """
    Health check para Kubernetes/monitoring
    Verifica que el servicio estÃ© vivo
    """
    return {
        "status": "healthy",
        "service": "pytorch-service",
    }

# ============================================
# PREDICCIÃ“N
# ============================================
@app.post("/predict")
async def predict(data: dict):
    """
    Endpoint principal de predicciÃ³n

    Args:
        data: Diccionario con datos de entrada

    Returns:
        Diccionario con predicciÃ³n
    """
    # AquÃ­ irÃ­a tu modelo ML real
    return {
        "prediction": "example_result",
        "confidence": 0.95,
        "model_version": "1.0.0",
    }
```

### 2.3 Probar los Endpoints

#### MÃ©todo 1: Con curl (Terminal)

```bash
# 1. Root endpoint
curl http://localhost:8000/

# Respuesta:
# {
#   "service": "pytorch-service",
#   "version": "1.0.0",
#   "status": "running"
# }

# 2. Health check
curl http://localhost:8000/health

# 3. PredicciÃ³n
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"data": "test"}'
```

#### MÃ©todo 2: Con Python

Crear archivo `test_pytorch.py`:

```python
import requests

# URL base del servicio
BASE_URL = "http://localhost:8000"

# Test 1: Root
print("ğŸ§ª Test 1: Root endpoint")
response = requests.get(f"{BASE_URL}/")
print(f"Status: {response.status_code}")
print(f"Response: {response.json()}")
print()

# Test 2: Health
print("ğŸ§ª Test 2: Health check")
response = requests.get(f"{BASE_URL}/health")
print(f"Status: {response.status_code}")
print(f"Response: {response.json()}")
print()

# Test 3: PredicciÃ³n
print("ğŸ§ª Test 3: PredicciÃ³n")
data = {"data": "test input"}
response = requests.post(f"{BASE_URL}/predict", json=data)
print(f"Status: {response.status_code}")
print(f"Response: {response.json()}")
```

Ejecutar:
```bash
python test_pytorch.py
```

#### MÃ©todo 3: Swagger UI (Navegador)

FastAPI genera documentaciÃ³n interactiva automÃ¡ticamente:

1. Abrir: http://localhost:8000/docs
2. Ver todos los endpoints
3. Probar directamente desde el navegador
4. Ver esquemas de request/response

### 2.4 Ver Logs en Tiempo Real

```bash
# Logs del servicio PyTorch
docker-compose logs -f pytorch-service

# Ver solo Ãºltimas 50 lÃ­neas
docker-compose logs --tail=50 pytorch-service

# Filtrar por ERROR
docker-compose logs pytorch-service | grep ERROR

# Ver mÃºltiples servicios
docker-compose logs -f pytorch-service tensorflow-service
```

**Interpretar logs:**
```
mlops-pytorch | INFO:     Started server process [1]
                â†‘ Nombre     â†‘ Level  â†‘ Mensaje
               contenedor
```

---

## ğŸ“– CapÃ­tulo 3: MLflow - Tracking de Experimentos

### 3.1 Â¿QuÃ© es MLflow?

MLflow es tu **laboratorio digital** para ML:

```
ğŸ”¬ EXPERIMENTO
â”œâ”€â”€ Run 1: learning_rate=0.001 â†’ accuracy=0.85
â”œâ”€â”€ Run 2: learning_rate=0.01  â†’ accuracy=0.92 âœ… Mejor
â””â”€â”€ Run 3: learning_rate=0.1   â†’ accuracy=0.78
```

**Problema sin MLflow:**
```
experimento_v1.ipynb
experimento_v2_final.ipynb
experimento_v2_final_REAL.ipynb
experimento_v3_ahora_si.ipynb
```

**Con MLflow:**
- âœ… Todos los experimentos organizados
- âœ… Comparar visualmente
- âœ… Reproducir cualquier run
- âœ… Compartir con el equipo

### 3.2 Componentes de MLflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             MLflow                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. TRACKING
   â†’ Registrar parÃ¡metros, mÃ©tricas, modelos

2. PROJECTS
   â†’ Empaquetar cÃ³digo reproducible

3. MODELS
   â†’ Gestionar modelos en producciÃ³n

4. REGISTRY
   â†’ Versionado central de modelos
```

### 3.3 Acceder a MLflow UI

1. Abrir: http://localhost:5000
2. VerÃ¡s el dashboard principal
3. Por ahora estÃ¡ vacÃ­o (sin experimentos)

**Elementos de la UI:**
- **Experiments**: Lista de experimentos
- **Runs**: Ejecuciones dentro de un experimento
- **Compare**: Comparar mÃºltiples runs
- **Models**: Modelos registrados

### 3.4 Primer Experimento con MLflow

Crear `scripts/my_first_experiment.py`:

```python
import mlflow
import mlflow.sklearn
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# ============================================
# CONFIGURAR MLFLOW
# ============================================
mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("iris-classification")

# ============================================
# CARGAR DATOS
# ============================================
print("ğŸ“Š Cargando datos...")
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)

# ============================================
# EXPERIMENTO
# ============================================
# Probar diferentes hiperparÃ¡metros
params_to_test = [
    {"n_estimators": 10, "max_depth": 3},
    {"n_estimators": 50, "max_depth": 5},
    {"n_estimators": 100, "max_depth": 10},
]

for params in params_to_test:
    # Iniciar un "run" en MLflow
    with mlflow.start_run(run_name=f"rf_{params['n_estimators']}"):

        print(f"\nğŸ§ª Entrenando con {params}")

        # ============================================
        # 1. LOG PARÃMETROS
        # ============================================
        mlflow.log_params(params)

        # ============================================
        # 2. ENTRENAR MODELO
        # ============================================
        model = RandomForestClassifier(**params, random_state=42)
        model.fit(X_train, y_train)

        # ============================================
        # 3. EVALUAR
        # ============================================
        train_acc = accuracy_score(y_train, model.predict(X_train))
        test_acc = accuracy_score(y_test, model.predict(X_test))

        # ============================================
        # 4. LOG MÃ‰TRICAS
        # ============================================
        mlflow.log_metric("train_accuracy", train_acc)
        mlflow.log_metric("test_accuracy", test_acc)

        # ============================================
        # 5. LOG MODELO
        # ============================================
        mlflow.sklearn.log_model(model, "random_forest_model")

        print(f"  âœ“ Train Accuracy: {train_acc:.4f}")
        print(f"  âœ“ Test Accuracy: {test_acc:.4f}")

print("\nğŸ‰ Â¡Experimentos completados!")
print("ğŸ‘‰ Ver resultados en: http://localhost:5000")
```

**Ejecutar:**
```bash
python scripts/my_first_experiment.py
```

**Ver en MLflow UI:**
1. Refresh http://localhost:5000
2. Ver experimento "iris-classification"
3. Ver los 3 runs
4. Compararlos visualmente

### 3.5 Comparar Runs en MLflow

En la UI de MLflow:

1. Seleccionar mÃºltiples runs (checkbox)
2. Click en "Compare"
3. Ver tabla de comparaciÃ³n:

```
Run             | n_estimators | max_depth | test_accuracy
----------------|--------------|-----------|---------------
rf_10           | 10           | 3         | 0.9333
rf_50           | 50           | 5         | 0.9667
rf_100          | 100          | 10        | 1.0000  âœ… Mejor
```

4. Ver grÃ¡ficas paralelas
5. Analizar diferencias

---

## ğŸ“– CapÃ­tulo 4: Prometheus y MÃ©tricas

### 4.1 Â¿QuÃ© es Prometheus?

Prometheus es un **sistema de monitoreo** que:
- Recolecta mÃ©tricas cada X segundos
- Almacena en base de datos de series temporales
- Permite hacer queries y alertas

**MÃ©tricas tÃ­picas:**
- ğŸ”¢ **Counters**: Cuentan eventos (requests totales)
- ğŸ“Š **Gauges**: Valores que suben/bajan (CPU usage)
- â±ï¸ **Histograms**: Distribuciones (latencia)

### 4.2 Acceder a Prometheus

1. Abrir: http://localhost:9090
2. Ver el dashboard
3. Ir a "Graph"

### 4.3 Queries BÃ¡sicas

En la UI de Prometheus, probar estas queries:

#### Query 1: Ver mÃ©tricas disponibles
```promql
{job="pytorch-service"}
```

#### Query 2: Requests por segundo
```promql
rate(requests_total[5m])
```

#### Query 3: Latencia promedio
```promql
avg(request_duration_seconds)
```

#### Query 4: Uso de CPU
```promql
process_cpu_seconds_total{job="pytorch-service"}
```

### 4.4 AÃ±adir MÃ©tricas Custom

Editar `src/main.py`:

```python
from prometheus_client import Counter, Histogram, make_asgi_app

# ============================================
# DEFINIR MÃ‰TRICAS
# ============================================
# Counter: Solo incrementa
requests_total = Counter(
    'api_requests_total',
    'Total API requests',
    ['method', 'endpoint']
)

# Histogram: Para latencias
request_duration = Histogram(
    'api_request_duration_seconds',
    'API request duration'
)

# ============================================
# USAR EN ENDPOINTS
# ============================================
@app.get("/")
async def root():
    # Incrementar contador
    requests_total.labels(method='GET', endpoint='/').inc()

    return {"service": "pytorch-service"}

@app.post("/predict")
async def predict(data: dict):
    # Medir tiempo
    with request_duration.time():
        # Tu cÃ³digo aquÃ­
        result = {"prediction": "example"}

    requests_total.labels(method='POST', endpoint='/predict').inc()
    return result

# ============================================
# ENDPOINT DE MÃ‰TRICAS
# ============================================
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)
```

**Ver mÃ©tricas:**
```bash
curl http://localhost:8000/metrics
```

VerÃ¡s algo como:
```
# HELP api_requests_total Total API requests
# TYPE api_requests_total counter
api_requests_total{endpoint="/",method="GET"} 42.0
api_requests_total{endpoint="/predict",method="POST"} 15.0

# HELP api_request_duration_seconds API request duration
# TYPE api_request_duration_seconds histogram
api_request_duration_seconds_bucket{le="0.005"} 10.0
api_request_duration_seconds_sum 1.234
api_request_duration_seconds_count 57.0
```

---

## ğŸ“– CapÃ­tulo 5: Trabajando con Datos

### 5.1 Estructura de Datos

```
mlops-project/
â”œâ”€â”€ data/              â† Datasets
â”‚   â”œâ”€â”€ raw/           â† Datos originales
â”‚   â”œâ”€â”€ processed/     â† Datos procesados
â”‚   â””â”€â”€ models/        â† Modelos guardados
```

### 5.2 Montar VolÃºmenes

En `docker-compose.yml`:

```yaml
pytorch-service:
  volumes:
    - ./data:/app/data      # Carpeta local â†’ carpeta en contenedor
    - ./models:/app/models
```

**Â¿QuÃ© significa esto?**
- Cambios en `./data` se ven en el contenedor
- Modelos guardados persisten despuÃ©s de detener el contenedor

### 5.3 Ejemplo: Guardar un Modelo

```python
import torch
import torch.nn as nn

# ============================================
# CREAR MODELO SIMPLE
# ============================================
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(10, 2)

    def forward(self, x):
        return self.linear(x)

# Instanciar
model = SimpleModel()

# ============================================
# GUARDAR MODELO
# ============================================
# Dentro del contenedor: /app/models/my_model.pth
# En tu mÃ¡quina: ./models/my_model.pth
torch.save(model.state_dict(), '/app/models/my_model.pth')

print("âœ… Modelo guardado en ./models/my_model.pth")
```

**Verificar en tu mÃ¡quina:**
```bash
ls -lh models/
# Debes ver my_model.pth
```

---

## ğŸ“– CapÃ­tulo 6: Proyecto PrÃ¡ctico

### 6.1 Objetivo

Crear un clasificador de imÃ¡genes:
1. Entrenar modelo con PyTorch
2. Registrar en MLflow
3. Exponer como API
4. Monitorear con Prometheus

### 6.2 Paso 1: Entrenar Modelo

Crear `scripts/train_image_classifier.py`:

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import mlflow
import mlflow.pytorch

# ============================================
# CONFIGURACIÃ“N
# ============================================
mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("image-classifier")

# ============================================
# PREPARAR DATOS
# ============================================
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Descargar MNIST
train_dataset = datasets.MNIST(
    root='./data',
    train=True,
    download=True,
    transform=transform
)

# DataLoader
train_loader = torch.utils.data.DataLoader(
    train_dataset,
    batch_size=64,
    shuffle=True
)

# ============================================
# MODELO
# ============================================
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = x.view(-1, 28*28)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# ============================================
# ENTRENAR
# ============================================
with mlflow.start_run(run_name="mnist-classifier"):

    # HiperparÃ¡metros
    params = {
        "epochs": 3,
        "lr": 0.001,
        "batch_size": 64
    }
    mlflow.log_params(params)

    # Modelo
    model = Net()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=params["lr"])

    # Training loop
    for epoch in range(params["epochs"]):
        running_loss = 0.0
        for i, (images, labels) in enumerate(train_loader):

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            if i % 100 == 99:
                avg_loss = running_loss / 100
                print(f'Epoch {epoch+1}, Batch {i+1}: Loss = {avg_loss:.4f}')
                mlflow.log_metric("loss", avg_loss, step=i)
                running_loss = 0.0

    # Guardar modelo
    mlflow.pytorch.log_model(model, "mnist_model")
    torch.save(model.state_dict(), './models/mnist_classifier.pth')

    print("âœ… Entrenamiento completado!")
```

**Ejecutar:**
```bash
python scripts/train_image_classifier.py
```

### 6.3 Paso 2: Cargar y Usar Modelo

Crear `scripts/predict_image.py`:

```python
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image

# ============================================
# DEFINIR MODELO (misma arquitectura)
# ============================================
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = x.view(-1, 28*28)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# ============================================
# CARGAR MODELO
# ============================================
model = Net()
model.load_state_dict(torch.load('./models/mnist_classifier.pth'))
model.eval()

print("âœ… Modelo cargado")

# ============================================
# PREPARAR IMAGEN
# ============================================
transform = transforms.Compose([
    transforms.Grayscale(),
    transforms.Resize((28, 28)),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# ============================================
# PREDECIR
# ============================================
def predict_digit(image_path):
    """Predice quÃ© dÃ­gito es una imagen"""

    # Cargar imagen
    image = Image.open(image_path)
    image = transform(image).unsqueeze(0)

    # Predecir
    with torch.no_grad():
        output = model(image)
        prediction = output.argmax(dim=1).item()
        confidence = torch.softmax(output, dim=1).max().item()

    return prediction, confidence

# Ejemplo de uso
if __name__ == "__main__":
    # Necesitas una imagen de prueba
    # prediction, confidence = predict_digit("test_image.png")
    # print(f"PredicciÃ³n: {prediction}, Confianza: {confidence:.2%}")

    print("ğŸ‘‰ Usa: predict_digit('path/to/image.png')")
```

### 6.4 Paso 3: Integrar en API

Editar `src/main.py` para aÃ±adir endpoint:

```python
from fastapi import FastAPI, File, UploadFile
import torch
from PIL import Image
import io

app = FastAPI()

# Cargar modelo al inicio
model = load_model()  # Tu funciÃ³n

@app.post("/predict/image")
async def predict_image(file: UploadFile = File(...)):
    """
    Predice quÃ© dÃ­gito contiene una imagen
    """
    # Leer imagen
    contents = await file.read()
    image = Image.open(io.BytesIO(contents))

    # Predecir
    prediction, confidence = predict_digit(image)

    return {
        "digit": prediction,
        "confidence": confidence,
        "model": "mnist-classifier"
    }
```

**Probar:**
```bash
curl -X POST http://localhost:8000/predict/image \
  -F "file=@test_digit.png"
```

---

## ğŸ“– CapÃ­tulo 7: Ejercicios PrÃ¡cticos

### Ejercicio 1: AÃ±adir Logging

Modificar `src/main.py` para aÃ±adir logs estructurados:

```python
import logging
import structlog

# Configurar logger
logging.basicConfig(level=logging.INFO)
logger = structlog.get_logger()

@app.post("/predict")
async def predict(data: dict):
    logger.info("prediction_request", data=data)

    # Tu cÃ³digo
    result = {"prediction": "example"}

    logger.info("prediction_response", result=result)
    return result
```

### Ejercicio 2: Cacheo con Redis

Usar Redis para cachear predicciones:

```python
import redis
import json
import hashlib

redis_client = redis.from_url("redis://redis:6379/0")

@app.post("/predict")
async def predict(data: dict):
    # Crear key del cache
    cache_key = hashlib.md5(
        json.dumps(data).encode()
    ).hexdigest()

    # Buscar en cache
    cached = redis_client.get(cache_key)
    if cached:
        logger.info("cache_hit", key=cache_key)
        return json.loads(cached)

    # Si no existe, calcular
    result = calculate_prediction(data)

    # Guardar en cache (1 hora)
    redis_client.setex(
        cache_key,
        3600,
        json.dumps(result)
    )

    return result
```

### Ejercicio 3: Rate Limiting

Limitar nÃºmero de requests por usuario:

```python
from fastapi import HTTPException
from collections import defaultdict
import time

# Contador de requests por IP
request_counts = defaultdict(list)
MAX_REQUESTS = 10  # por minuto

@app.post("/predict")
async def predict(request: Request, data: dict):
    # Obtener IP del cliente
    client_ip = request.client.host

    # Limpiar requests antiguos
    now = time.time()
    request_counts[client_ip] = [
        t for t in request_counts[client_ip]
        if now - t < 60
    ]

    # Verificar lÃ­mite
    if len(request_counts[client_ip]) >= MAX_REQUESTS:
        raise HTTPException(
            status_code=429,
            detail="Too many requests"
        )

    # Registrar request
    request_counts[client_ip].append(now)

    # Procesar
    return {"prediction": "ok"}
```

---

## ğŸ“– CapÃ­tulo 8: Resumen y PrÃ³ximos Pasos

### âœ… Has Completado el Tomo 2

Ahora sabes:
- âœ… CÃ³mo funcionan contenedores Docker
- âœ… AnatomÃ­a de Dockerfiles
- âœ… Usar PyTorch Service
- âœ… Registrar experimentos en MLflow
- âœ… Trabajar con FastAPI
- âœ… Monitorear con Prometheus
- âœ… Entrenar y servir modelos
- âœ… Integrar logging y caching

### ğŸ¯ Siguiente: Tomo 3

En el **Tomo 3: Event-Driven y RAG** aprenderÃ¡s:
- ğŸ° Event-Driven Architecture con RabbitMQ
- ğŸ“¨ ComunicaciÃ³n asÃ­ncrona entre servicios
- ğŸ” RAG (Retrieval Augmented Generation)
- ğŸ“š ChromaDB y bÃºsqueda semÃ¡ntica
- ğŸ¤– Construir un chatbot inteligente

### ğŸ“ Checklist de ConsolidaciÃ³n

Antes de continuar, verifica que puedes:

- â˜ Construir una imagen Docker
- â˜ Modificar cÃ³digo y ver cambios (hot reload)
- â˜ Hacer requests a la API
- â˜ Ver logs de un servicio
- â˜ Entrenar un modelo y registrarlo en MLflow
- â˜ Comparar experimentos en MLflow UI
- â˜ Ver mÃ©tricas en Prometheus
- â˜ Guardar y cargar modelos

---

## ğŸ“š Ãndice de Tomos

- âœ… **Tomo 1: Fundamentos y Setup**
- âœ… **Tomo 2: Docker y Servicios BÃ¡sicos** â† EstÃ¡s aquÃ­
- â­ï¸ **Tomo 3: Event-Driven y RAG**
- â­ï¸ **Tomo 4: Deployment con Ansible**
- â­ï¸ **Tomo 5: Kubernetes y ProducciÃ³n**

---

**Â¡Excelente progreso! ğŸ‰ Listo para el Tomo 3! ğŸš€**
